{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.applications.vgg16 import VGG16 as cnn\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom keras.layers import Input, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","execution_count":255,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/train'\nval_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/val'\ntest_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/test'\n# len(os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA'))","execution_count":256,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls /kaggle/input/chest-xray-pneumonia/chest_xray/test","execution_count":257,"outputs":[{"output_type":"stream","text":"\u001b[0m\u001b[01;34mNORMAL\u001b[0m/  \u001b[01;34mPNEUMONIA\u001b[0m/\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(os.listdir(train_dir + '/NORMAL')) + len(os.listdir(train_dir + '/PNEUMONIA'))\nprint(nb_train_samples)\nnb_val_samples  = len(os.listdir(val_dir + '/NORMAL')) + len(os.listdir(val_dir + '/PNEUMONIA'))\nprint(nb_val_samples)\nnb_test_samples  = len(os.listdir(test_dir + '/NORMAL')) + len(os.listdir(val_dir + '/PNEUMONIA'))\nprint(nb_test_samples)","execution_count":258,"outputs":[{"output_type":"stream","text":"5216\n16\n242\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1./255,\n                                      shear_range=0.2,zoom_range=0.2,\n                                      horizontal_flip=True)\n\nval_datagen = ImageDataGenerator(rescale = 1./255)\n\ntest_datagen = ImageDataGenerator(rescale=1./ 255)\n\nbatch_size = 10\nimg_width = 150\nimg_height = 150\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,batch_size = batch_size,\n                                                    class_mode = 'binary', \n                                                    target_size =(img_width, img_height))\n\nval_generator = val_datagen.flow_from_directory(val_dir,batch_size = batch_size,\n                                                    class_mode = 'binary', \n                                                    target_size =(img_width, img_height))\n\n\n\ntest_generator = test_datagen.flow_from_directory(test_dir,batch_size=batch_size,                                              \n                                                class_mode='binary',\n                                                target_size=(img_width, img_height))\n\n","execution_count":259,"outputs":[{"output_type":"stream","text":"Found 5216 images belonging to 2 classes.\nFound 16 images belonging to 2 classes.\nFound 624 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_train,y_train=train_generator.next()\nx_test,y_test=test_generator.next(624)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n# Define path to the data directory\ndata_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\nimport cv2\nfrom keras.utils import to_categorical\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'\n\n#Get the path to the sub-directories\nnormal_cases_dir = train_dir / 'NORMAL'\npneumonia_cases_dir = train_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\nimport numpy as np\n\n# Some images are in grayscale while majority of them contains 3 channels. So, if the image is grayscale, we will convert into a image with 3 channels.\n# We will normalize the pixel values and resizing all the images to 224x224 \n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150)).astype(np.float32)/255.0\n    label = to_categorical(0, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.0\n    label = to_categorical(1, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n# Convert the list into numpy arrays\nx_train = np.array(valid_data)\ny_train = np.array(valid_labels)\n","execution_count":260,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":261,"outputs":[{"output_type":"execute_result","execution_count":261,"data":{"text/plain":"(5216, 150, 150, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n# Define path to the data directory\ndata_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\nimport cv2\nfrom keras.utils import to_categorical\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'\n\n#Get the path to the sub-directories\nnormal_cases_dir = test_dir / 'NORMAL'\npneumonia_cases_dir = test_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\nimport numpy as np\n\n# Some images are in grayscale while majority of them contains 3 channels. So, if the image is grayscale, we will convert into a image with 3 channels.\n# We will normalize the pixel values and resizing all the images to 224x224 \n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150)).astype(np.float32)/255.0\n    label = to_categorical(0, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.0\n    label = to_categorical(1, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n# Convert the list into numpy arrays\nx_test = np.array(valid_data)\ny_test = np.array(valid_labels)\n","execution_count":262,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_test.shape","execution_count":263,"outputs":[{"output_type":"execute_result","execution_count":263,"data":{"text/plain":"(624, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_shape = Input(shape=(150,150,3))\nmodel = cnn(weights = 'imagenet',input_shape=(150,150,3), include_top=False)\n\nmodel.summary()","execution_count":264,"outputs":[{"output_type":"stream","text":"Model: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_20 (InputLayer)        [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now we will freeze all layers of our pretrained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:\n      layer.trainable = False","execution_count":265,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlast_output = model.output","execution_count":266,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)               \n# Add a final sigmoid layer for classification\nx = layers.Dense(2, activation='softmax')(x)           \n\nmodel = Model(model.input, x) \n\nmodel.summary()","execution_count":267,"outputs":[{"output_type":"stream","text":"Model: \"model_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_20 (InputLayer)        [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten_8 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 1024)              8389632   \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 2)                 2050      \n=================================================================\nTotal params: 23,106,370\nTrainable params: 8,391,682\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model=cnn(weights=None, include_top=True,classes=2,input_shape=(150,150,3))\nmodel.summary()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sgd=SGD(lr=0.01,momentum=0.9,nesterov=True)\n#model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n\nmodel.compile(optimizer = Adam(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics = ['acc'])","execution_count":268,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"history = model.fit_generator(\n            train_generator,\n            validation_data = val_generator,\n            steps_per_epoch = nb_train_samples // batch_size,\n            epochs = 1,\n            validation_steps = nb_val_samples // batch_size,\n            verbose = 1)\n\nhis=model.fit(x_train, y_train, validation_split=0.2, epochs=1, batch_size=10)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory=model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=5, batch_size=10)\n","execution_count":269,"outputs":[{"output_type":"stream","text":"Train on 5216 samples, validate on 624 samples\n5216/5216 [==============================] - 15s 3ms/sample - loss: 0.1366 - acc: 0.9459 - val_loss: 0.4260 - val_acc: 0.8670\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","execution_count":270,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"score = model.evaluate(x_test, y_test,batch_size=10)\nprint(score[1])\n# evaluate the model\n#scores = model.evaluate_generator(test_generator)\n#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"/kaggle/working/vgg16model.h5\")","execution_count":271,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(score[1]*100)","execution_count":272,"outputs":[{"output_type":"stream","text":"86.69871687889099\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras import activations, constraints, initializers, regularizers\nfrom keras.engine import InputSpec, Layer\nfrom keras.layers import Dense\nfrom keras.utils import conv_utils\n\n\nclass FactorizedDense(Layer):\n    \"\"\"Just your regular densely-connected NN layer.\n    This layer based on `keras.layers.core.Dense` and behave like it.\n    `FactorizedDense` implements the operation:\n    `output = activation(dot(dot(input, pre_kernel), post_kernel) + bias)`\n    where `activation` is the element-wise activation function\n    passed as the `activation` argument, `pre_kernel` and `post_kernel` is a weights matrix\n    created by the layer, and `bias` is a bias vector created by the layer\n    (only applicable if `use_bias` is `True`).\n    Note: if the input to the layer has a rank greater than 2, then\n    it is flattened prior to the initial dot product with `pre_kernel`.\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        components: Positive integer or None, the size of internal components.\n            If given None, the output is calculated as the same manner as `Dense` layer.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        pre_kernel_initializer: Initializer for the `kernel` weights matrix\n            (see [initializers](../initializers.md)).\n        post_kernel_initializer: Initializer for the `kernel` weights matrix\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        pre_kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        post_kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n    # Input shape\n        nD tensor with shape: `(batch_size, ..., input_dim)`.\n        The most common situation would be\n        a 2D input with shape `(batch_size, input_dim)`.\n    # Output shape\n        nD tensor with shape: `(batch_size, ..., units)`.\n        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n        the output would have shape `(batch_size, units)`.\n    \"\"\"\n    target_layer_types = [Dense]\n\n    def __init__(self, units, components,\n                 activation=None,\n                 use_bias=True,\n\n                 pre_kernel_initializer='glorot_uniform',\n                 post_kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n\n                 pre_kernel_regularizer=None,\n                 post_kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n\n                 pre_kernel_constraint=None,\n                 post_kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super(FactorizedDense, self).__init__(**kwargs)\n        self.units = units\n        self.components = components\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.pre_kernel_initializer = initializers.get(pre_kernel_initializer)\n        self.post_kernel_initializer = initializers.get(post_kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.pre_kernel_regularizer = regularizers.get(pre_kernel_regularizer)\n        self.post_kernel_regularizer = regularizers.get(post_kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.pre_kernel_constraint = constraints.get(pre_kernel_constraint)\n        self.post_kernel_constraint = constraints.get(post_kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.input_spec = InputSpec(min_ndim=2)\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        is_factorized = self.components is not None\n\n        if is_factorized:\n            shape = (input_dim, self.components)\n        else:\n            shape = (input_dim, self.units)\n\n        self.pre_kernel = self.add_weight(shape,\n                                          initializer=self.pre_kernel_initializer,\n                                          name='pre_kernel',\n                                          regularizer=self.pre_kernel_regularizer,\n                                          constraint=self.pre_kernel_constraint)\n\n        if not is_factorized:\n            self.post_kernel = None\n        else:\n            self.post_kernel = self.add_weight((self.components, self.units),\n                                               initializer=self.post_kernel_initializer,\n                                               name='kernel',\n                                               regularizer=self.post_kernel_regularizer,\n                                               constraint=self.post_kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n\n    def call(self, inputs):\n        h = K.dot(inputs, self.pre_kernel)\n        if self.post_kernel is not None:\n            h = K.dot(h, self.post_kernel)\n        if self.use_bias:\n            h = K.bias_add(h, self.bias)\n        if self.activation is not None:\n            h = self.activation(h)\n        return h\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        assert input_shape[-1]\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)\n\n    def get_config(self):\n        config = {\n            'units': self.units,\n            'activation': activations.serialize(self.activation),\n            'components': self.components,\n            'use_bias': self.use_bias,\n\n            'pre_kernel_initializer': initializers.serialize(self.pre_kernel_initializer),\n            'post_kernel_initializer': initializers.serialize(self.post_kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n\n            'pre_kernel_regularizer': regularizers.serialize(self.pre_kernel_regularizer),\n            'post_kernel_regularizer': regularizers.serialize(self.post_kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n\n            'pre_kernel_constraint': constraints.serialize(self.pre_kernel_constraint),\n            'post_kernel_constraint': constraints.serialize(self.post_kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(FactorizedDense, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass FactorizedConv2DTucker(Layer):\n    \"\"\"2D convolution layer with tucker decomposition.\n    This layer is based on `keras.layers.convolution.Conv2D` and behave like it.\n    The difference is the kernel is factorized by tucker decomposition.\n    If `use_bias` is True, a bias vector is created and added to the outputs.\n    Finally, if `activation` is not `None`, it is applied to the outputs as well.\n    When using this layer as the first layer in a model,\n    provide the keyword argument `input_shape`\n    (tuple of integers, does not include the sample axis),\n    e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n    in `data_format=\"channels_last\"`.\n    # Arguments\n        filters: Integer, the dimensionality of the output space\n            (i.e. the number output of filters in the convolution).\n        kernel_size: An integer or tuple/list of 2 integers, specifying the\n            width and height of the 2D convolution window.\n            Can be a single integer to specify the same value for\n            all spatial dimensions.\n        input_components: Integer or None, the number of components\n            of kernel for the input channel axis. If given None, the\n            factorization of input side is skipped.\n        output_components: Integer or None, the number of components\n            of kernel for the output channel axis. If given None, the\n            factorization of output side is skipped.\n        strides: An integer or tuple/list of 2 integers,\n            specifying the strides of the convolution along the width and height.\n            Can be a single integer to specify the same value for\n            all spatial dimensions.\n            Specifying any stride value != 1 is incompatible with specifying\n            any `dilation_rate` value != 1.\n        padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n        data_format: A string,\n            one of `channels_last` (default) or `channels_first`.\n            The ordering of the dimensions in the inputs.\n            `channels_last` corresponds to inputs with shape\n            `(batch, width, height, channels)` while `channels_first`\n            corresponds to inputs with shape\n            `(batch, channels, width, height)`.\n            It defaults to the `image_data_format` value found in your\n            Keras config file at `~/.keras/keras.json`.\n            If you never set it, then it will be \"channels_last\".\n        dilation_rate: an integer or tuple/list of 2 integers, specifying\n            the dilation rate to use for dilated convolution.\n            Can be a single integer to specify the same value for\n            all spatial dimensions.\n            Currently, specifying any `dilation_rate` value != 1 is\n            incompatible with specifying any stride value != 1.\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you don't specify anything, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        pre_kernel_initializer: Initializer for the `kernel` weights matrix\n            (see [initializers](../initializers.md)).\n        kernel_initializer: Initializer for the `kernel` weights matrix\n            (see [initializers](../initializers.md)).\n        post_kernel_initializer: Initializer for the `kernel` weights matrix\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        pre_kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        post_kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        pre_kernel_constraint: Constraint function applied to the kernel matrix\n            (see [constraints](../constraints.md)).\n        kernel_constraint: Constraint function applied to the kernel matrix\n            (see [constraints](../constraints.md)).\n        post_kernel_constraint: Constraint function applied to the kernel matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n    # Input shape\n        4D tensor with shape:\n        `(samples, channels, rows, cols)` if data_format='channels_first'\n        or 4D tensor with shape:\n        `(samples, rows, cols, channels)` if data_format='channels_last'.\n    # Output shape\n        4D tensor with shape:\n        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n        or 4D tensor with shape:\n        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n        `rows` and `cols` values might have changed due to padding.\n    \"\"\"\n\n    def __init__(self,\n                 filters,\n                 kernel_size,\n                 input_components=None,\n                 output_components=None,\n\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation=None,\n                 use_bias=True,\n\n                 pre_kernel_initializer='glorot_uniform',\n                 kernel_initializer='glorot_uniform',\n                 post_kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n\n                 pre_kernel_regularizer=None,\n                 kernel_regularizer=None,\n                 post_kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n\n                 pre_kernel_constraint=None,\n                 kernel_constraint=None,\n                 post_kernel_constraint=None,\n                 bias_constraint=None,\n\n                 **kwargs):\n        super(FactorizedConv2DTucker, self).__init__(**kwargs)\n        rank = 2\n        self.rank = rank\n        self.input_components = input_components\n        self.output_components = output_components\n        self.filters = filters\n        self.output_components = output_components\n\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.pre_kernel_initializer = initializers.get(pre_kernel_initializer)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.post_kernel_initializer = initializers.get(post_kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.pre_kernel_regularizer = regularizers.get(pre_kernel_regularizer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.post_kernel_regularizer = regularizers.get(post_kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.pre_kernel_constraint = constraints.get(pre_kernel_constraint)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.post_kernel_constraint = constraints.get(post_kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.input_spec = InputSpec(ndim=rank + 2)  # batch, H, W, C\n\n    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n\n        input_dim = input_shape[channel_axis]\n        if self.input_components is None:\n            input_components = input_dim\n        else:\n            input_components = self.input_components\n        if self.output_components is None:\n            output_components = self.filters\n        else:\n            output_components = self.output_components\n        kernel_shape = self.kernel_size + (input_components, output_components)\n\n        if self.input_components is None:\n            self.pre_kernel = None\n        else:\n            pre_kernel_shape = (1, 1) + (input_dim, self.input_components)\n            self.pre_kernel = self.add_weight(pre_kernel_shape,\n                                              initializer=self.pre_kernel_initializer,\n                                              name='pre_kernel',\n                                              regularizer=self.pre_kernel_regularizer,\n                                              constraint=self.pre_kernel_constraint)\n\n        self.kernel = self.add_weight(kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        if self.output_components is None:\n            self.post_kernel = None\n        else:\n            post_kernel_shape = (1, 1) + (self.output_components, self.filters)\n            self.post_kernel = self.add_weight(post_kernel_shape,\n                                               initializer=self.post_kernel_initializer,\n                                               name='post_kernel',\n                                               regularizer=self.post_kernel_regularizer,\n                                               constraint=self.post_kernel_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight((self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True\n\n    def call(self, inputs):\n        h = inputs\n        if self.pre_kernel is not None:\n            h = K.conv2d(\n                h,\n                self.pre_kernel,\n                strides=(1, 1),\n                padding='valid',\n                data_format=self.data_format,\n                dilation_rate=(1, 1),\n            )\n        h = K.conv2d(\n            h,\n            self.kernel,\n            strides=self.strides,\n            padding=self.padding,\n            data_format=self.data_format,\n            dilation_rate=self.dilation_rate,\n        )\n        if self.post_kernel is not None:\n            h = K.conv2d(\n                h,\n                self.post_kernel,\n                strides=(1, 1),\n                padding='valid',\n                data_format=self.data_format,\n                dilation_rate=(1, 1),\n            )\n\n        outputs = h\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_last':\n            space = input_shape[1:-1]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n        if self.data_format == 'channels_first':\n            space = input_shape[2:]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0], self.filters) + tuple(new_space)\n\n    def get_config(self):\n        config = {\n            'input_components': self.input_components,\n            'output_components': self.output_components,\n            'filters': self.filters,\n\n            'kernel_size': self.kernel_size,\n            'strides': self.strides,\n            'padding': self.padding,\n            'data_format': self.data_format,\n            'dilation_rate': self.dilation_rate,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n\n            'pre_kernel_initializer': initializers.serialize(self.pre_kernel_initializer),\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'post_kernel_initializer': initializers.serialize(self.post_kernel_initializer),\n            'bias_initializer': initializers.serialize(self.kernel_initializer),\n\n            'pre_kernel_regularizer': regularizers.serialize(self.pre_kernel_regularizer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'post_kernel_regularizer': regularizers.serialize(self.post_kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n\n            'pre_kernel_constraint': constraints.serialize(self.pre_kernel_constraint),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'post_kernel_constraint': constraints.serialize(self.post_kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(FactorizedConv2DTucker, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\ncustom_layers = {\n    'FactorizedConv2DTucker': FactorizedConv2DTucker,\n    'FactorizedDense': FactorizedDense,\n}","execution_count":273,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Any, Dict, List\n\nfrom keras.engine.topology import Layer, Node\n\n\ndef swap_layer_connection(old_layer: Layer, new_layer: Layer) -> None:\n    '''connect nodes of calc graph for new_layer and disconnect ones for old_layers\n    Keras manages calculation graph by nodes which hold connection between\n    layres. To swap old layer and new layer, it is required to delete nodes\n    of old layer and to create new nodes of new layer.\n    :arg old_layer: Old layer. The connection to/from this layer will be removed.\n    :arg new_layer: New layer. The connection to/from old layer will be connected to/from\n        this layer.\n    :return: None\n    '''\n\n    # the set of inbound layer which have old outbound_node\n    inbound_layers = set()\n\n    # create new inbound nodes\n    for node in old_layer._inbound_nodes:  # type: Node\n        Node(\n            new_layer, node.inbound_layers,\n            node.node_indices, node.tensor_indices,\n            node.input_tensors, node.output_tensors,\n            node.input_masks, node.output_masks,\n            node.input_shapes, node.output_shapes,\n        )\n        inbound_layers.union(set(node.inbound_layers))\n\n    # remove old outbound node of inbound layers\n    for layer in inbound_layers:  # type: Layer\n        old_nodes = filter(\n            lambda n: n.outbound_layer == old_layer,\n            layer._outbound_nodes,\n        )\n        for n in old_nodes:  # type: Node\n            layer._outbound_nodes.remove(n)\n\n    # the set of outbound layer which have old inbound_nodes\n    outbound_layers = set()\n    # create new outbound nodes\n    for node in old_layer._outbound_nodes:  # type: Node\n        layers = list(node.inbound_layers)\n        while old_layer in layers:\n            idx = layers.index(old_layer)\n            layers[idx] = new_layer\n        Node(\n            node.outbound_layer, layers,\n            node.node_indices, node.tensor_indices,\n            node.input_tensors, node.output_tensors,\n            node.input_masks, node.output_masks,\n            node.input_shapes, node.output_shapes,\n        )\n        outbound_layers.add(node.outbound_layer)\n\n    # remove old inbound_node of outbound layers\n    for layer in outbound_layers:  # type: Layer\n        old_nodes = filter(\n            lambda n: old_layer in n.inbound_layers,\n            layer._inbound_nodes,\n        )\n        for n in old_nodes:\n            layer._inbound_nodes.remove(n)\n\n\ndef convert_config(\n        base_config: Dict[str, Any],\n        ignore_args: List[str],\n        converts: Dict[str, List[str]],\n        new_kwargs: Dict[str, Any]\n) -> Dict[str, Any]:\n    '''convert old layer's config to new layer's config.\n    :param base_config: Base config. Generally a config of old layer.\n    :param ignore_args: Ignore arg names. Not required arg names in new layer,\n        though them is required in old layer.\n    :param converts: Ignore name conversion dictionary, whose key is old layer's\n        arg name in base_config, and whose value is new layer's arg names(list).\n    :param new_kwargs: The new kwargs.\n    :return: Converted config.\n    '''\n    kwargs = {}\n    for k, v in base_config.items():\n        if k in ignore_args:\n            continue\n        elif k in converts:\n            for new_k in converts[k]:\n                kwargs[new_k] = v\n        else:\n            kwargs[k] = v\n    kwargs.update(new_kwargs)\n    return kwargs","execution_count":274,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import List, Optional, Type\n\nfrom keras.layers import Layer\n\n\nclass Factorizer:\n    factorize_target_layers = []  # type: List[Type[Layer]]\n\n    @classmethod\n    def compress(cls, layer: Layer, acceptable_error: float) -> Optional[Layer]:\n        \"\"\"try to compress the layer under acceptable_error.\n        Outputs compressed layer if compression succeeded. If not, return None.\n        :param layer:\n        :param acceptable_error:\n        :return:\n        \"\"\"\n        raise NotImplementedError","execution_count":275,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport math\nfrom typing import Optional, Tuple\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.engine import Layer\nfrom keras.layers import Dense\nfrom sklearn.utils.extmath import randomized_svd\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass SVDFactorizer(Factorizer):\n    factorize_target_layers = [Dense]\n\n    @staticmethod\n    def _factorize(W: np.ndarray, components: int):\n        \"\"\"\n        :param W: I x O\n        :param components: K\n        :return:\n            U: I x K\n            V: K x O\n        \"\"\"\n        u, s, v = randomized_svd(W, components)\n        scale = np.diag(np.sqrt(s))\n        U, V = u.dot(scale).astype(W.dtype), scale.dot(v).astype(W.dtype)\n        return U, V\n\n    @staticmethod\n    def _calc_error(W: np.ndarray, U: np.ndarray, V: np.ndarray):\n        '''\n        :param W: I x O\n        :param U: I x K\n        :param V: K x O\n        :return:\n        '''\n        elemental_error = np.abs(W - U.dot(V))\n        error_bound = np.mean(elemental_error) / np.mean(np.abs(W))\n        return error_bound\n\n    @classmethod\n    def compress(cls, old_layer: Dense, acceptable_error: float) -> Optional[Layer]:\n        '''compress old_layer under acceptable error using SVD.\n        If it can't reduce the number of parameters, returns None,\n        :param old_layer:\n        :param acceptable_error:\n        :return:\n        '''\n        W = K.get_value(old_layer.kernel)\n        logger.debug('factorization start W.shape:{}'.format(W.shape))\n\n        max_comps = math.floor(np.size(W) / sum(W.shape))\n        U, V = cls._factorize(W, max_comps)\n        if cls._calc_error(W, U, V) >= acceptable_error:\n            # Factorizer can't reduce the number of parameters in acceptable error by SVD.\n            # So, this factorizer failed compression.\n            return None\n\n        U, V = cls._compress_in_acceptable_error(\n            W, acceptable_error,\n            start_param_range=range(1, max_comps),\n        )\n        components = U.shape[-1]\n\n        base_config = old_layer.get_config()\n\n        new_config = convert_config(\n            base_config,\n            ignore_args=[\n                'kernel_constraint',\n            ],\n            converts={\n                'kernel_regularizer': [\n                    'pre_kernel_regularizer',\n                    'post_kernel_regularizer',\n                ],\n                'kernel_initializer': [\n                    'pre_kernel_initializer',\n                    'post_kernel_initializer',\n                ]\n            },\n            new_kwargs={\n                'components': components,\n            },\n        )\n\n        new_layer = FactorizedDense(**new_config)\n        new_layer.build(old_layer.get_input_shape_at(0))  # to initialize weight variables\n\n        K.set_value(new_layer.pre_kernel, U)\n        K.set_value(new_layer.post_kernel, V)\n\n        return new_layer\n\n    @classmethod\n    def _compress_in_acceptable_error(cls, W, acceptable_error: float, start_param_range: range) \\\n            -> Tuple[np.ndarray, np.ndarray]:\n        param_range = start_param_range\n        while len(param_range) > 0:  # while not (param_range.start == param_range.stop)\n            logger.debug('current param_range:{}'.format(param_range))\n            if len(param_range) == 1:\n                ncomp = param_range.start\n            else:\n                ncomp = round((param_range.start + param_range.stop) / 2)\n\n            U, V = cls._factorize(W, ncomp)\n            error = cls._calc_error(W, U, V)\n\n            if error <= acceptable_error:  # smallest ncomp is equal to or smaller than ncomp\n                # On the assumption that `error` monotonically decreasing by increasing ncomp\n                logger.debug('under acceptable error ncomp:{} threshold:{} error:{}'.format(\n                    ncomp, acceptable_error, error,\n                ))\n                param_range = range(param_range.start, ncomp)\n            else:  # the best is larger than ncomp\n                logger.debug('over acceptable error ncomp:{} threshold:{} error:{}'.format(\n                    ncomp, acceptable_error, error,\n                ))\n                param_range = range(ncomp + 1, param_range.stop)\n\n        # param_range.start == param_range.stop\n        smallest_ncomp = param_range.start\n        logger.debug('smallest_ncomp:{}, W.shape:{} compress_rate:{}'.format(\n            smallest_ncomp, W.shape, sum(W.shape) * smallest_ncomp / np.size(W),\n        ))\n        U, V = cls._factorize(W, smallest_ncomp)\n        return U, V","execution_count":276,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport logging\nimport math\nfrom queue import PriorityQueue\nfrom typing import Optional, Tuple\n\nimport numpy as np\nfrom keras import backend as K\nfrom keras.layers import Conv2D\n\nfrom sklearn.utils.extmath import randomized_svd\n\nlogger = logging.getLogger(__name__)\n\n__all__ = ['TuckerFactorizer']\n\n\nclass ProblemData:\n    '''Parameter search problem data structure\n    '''\n\n    def __init__(self, x_range: range, y_range: range):\n        self.x_range = x_range\n        self.y_range = y_range\n\n    def __str__(self):\n        return '<Problem x_range={} y_range={}>'.format(\n            self.x_range, self.y_range,\n        )\n\n    def __lt__(self, other: 'ProblemData'):\n        return self.diag_length < other.diag_length\n\n    def __eq__(self, other: 'ProblemData'):\n        return self.x_range == other.x_range and self.y_range == other.y_range\n\n    @property\n    def diag_length(self):\n        return math.sqrt(len(self.x_range) ** 2 + len(self.y_range) ** 2)\n\n\nclass Tucker:\n    '''Pure tucker decomposition functions\n    '''\n\n    @classmethod\n    def factorize(cls, W: np.ndarray, in_comps: Optional[int], out_comps: Optional[int]) \\\n            -> Tuple[np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"pure tucker decomposition\n        :param W: W x H x I x O\n        :param in_comps: N\n        :param out_comps:  M\n        :return:\n            C: W x H x N x M,\n            U_in: I x N\n            U_out: O x M\n        \"\"\"\n\n        if in_comps is None:\n            U_in = None\n        else:\n            U_in, _, _ = randomized_svd(cls._flatten(W, 2), in_comps)\n            U_in = U_in.astype(W.dtype)\n\n        if out_comps is None:\n            U_out = None\n        else:\n            U_out, _, _ = randomized_svd(cls._flatten(W, 3), out_comps)\n            U_out = U_out.astype(W.dtype)\n\n        C = W.copy()\n\n        if U_in is not None:\n            C = np.einsum('whio,in->whno', C, U_in)\n\n        if U_out is not None:\n            C = np.einsum('whno,om->whnm', C, U_out)\n\n        C = C.astype(W.dtype)\n\n        return C, U_in, U_out\n\n    @staticmethod\n    def _get_matrix(W: np.ndarray, i: int, axis: int) -> np.ndarray:\n        '''util function for tucker decomposition\n        :param W:\n        :param i:\n        :param axis:\n        :return:\n        '''\n        sli = [slice(None) for _ in range(W.ndim)]\n        sli[axis] = i\n        return W[sli]\n\n    @classmethod\n    def _flatten(cls, W: np.ndarray, axis: int) -> np.ndarray:\n        '''util function for tucker decomposition\n        :param W:\n        :param axis:\n        :return:\n        '''\n        dim = 1\n        dims = []\n        for i, v in enumerate(W.shape):\n            if i != axis:\n                dim *= v\n                dims.append(v)\n        res = np.zeros((W.shape[axis], dim))\n        for i in range(W.shape[axis]):\n            res[i] = cls._get_matrix(W, i, axis).ravel()\n        return res\n\n\nclass TuckerParamSearcher:\n    '''tucker decomposition parameter searcher\n    '''\n\n    def __init__(self, W: np.ndarray):\n        width, height, in_dim, out_dim = W.shape\n        self.width = width\n        self.height = height\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.W = W\n\n        self.best_point = None\n        self.best_param_num = width * height * in_dim * out_dim\n\n        self.prob_queue = PriorityQueue()\n\n    def add_problem(self, prob: ProblemData):\n        param_num = self.calc_min_param_num_by(prob)\n        self.prob_queue.put((param_num, prob))\n\n    def calc_min_param_num_by(self, prob: ProblemData):\n        res = []\n        for in_comp, out_comp in itertools.product(\n                [prob.x_range.start, prob.x_range.stop],\n                [prob.y_range.start, prob.y_range.stop],\n        ):\n            res.append(self.calc_param_num(in_comp, out_comp))\n        return min(res)\n\n    def calc_param_num(self, in_comp: int, out_comp: int):\n        params = self.width * self.height * in_comp * out_comp\n        if in_comp != self.in_dim:  # compression in input channel\n            params += self.in_dim * in_comp\n        if out_comp != self.out_dim:  # compression in output channel\n            params += self.out_dim * out_comp\n        return params\n\n    def update_best_point_if_needed(self, in_comp, out_comp):\n        current_param_num = self.calc_param_num(in_comp, out_comp)\n        if current_param_num < self.best_param_num:\n            self.best_point = (in_comp, out_comp)\n            self.best_param_num = current_param_num\n            logger.debug('update best_point={} best_param_num={}'.format(\n                self.best_point, self.best_param_num,\n            ))\n\n    def factorize_in_acceptable_error(self, acceptable_error: float) \\\n            -> Tuple[np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        :param W: W x H x I x O\n        :param acceptable_error:\n        :param initial_problems: initial problems\n        :return:\n            C: W x H x N x M\n            U_in: I x N\n            U_out: O x M\n        \"\"\"\n\n        # Search N and M whose the number of parameter is the smallest\n        # under acceptable error.\n\n        # This algorithm is based on divide and conquer algorithm and\n        # based on the assumption that `error` monotonically decrease by increasing N or M.\n\n        width, height, in_dim, out_dim = self.width, self.height, self.in_dim, self.out_dim\n\n        self.add_problem(ProblemData(range(in_dim, in_dim), range(out_dim, out_dim)))\n        if 2 <= in_dim and 2 <= out_dim:\n            self.add_problem(ProblemData(range(1, in_dim - 1), range(1, out_dim - 1)))\n        if 1 <= in_dim:\n            self.add_problem(ProblemData(range(in_dim, in_dim), range(1, out_dim - 1)))\n        if 1 <= out_dim:\n            self.add_problem(ProblemData(range(1, in_dim - 1), range(out_dim, out_dim)))\n\n        while not self.prob_queue.empty():\n            _, current_prob = self.prob_queue.get()  # type: ProblemData\n\n            if self.best_param_num < self.calc_min_param_num_by(current_prob):\n                logger.debug('no more best_param_num:{} prob:{}'.format(self.best_param_num, current_prob))\n                break\n\n            if len(current_prob.x_range) == 0 and len(current_prob.y_range) == 0:\n                self.update_best_point_if_needed(current_prob.x_range.start, current_prob.y_range.start)\n                continue\n\n            logger.debug('current queue.size:{} prob:{}'.format(\n                self.prob_queue.qsize(), current_prob,\n            ))\n\n            result = self._find_edge_point(\n                acceptable_error, current_prob,\n            )  # type: Optional[Tuple[int,int]]\n\n            logger.debug('result={} prob={}'.format(\n                result, current_prob\n            ))\n\n            if result is None:\n                continue\n\n            x, y = result  # type: Tuple[int, int]\n            self.update_best_point_if_needed(x, y)\n\n            # divide current problem to sub-problems\n\n            if len(current_prob.x_range) == 0 or len(current_prob.y_range) == 0:\n                # X.\n                #  Y\n                logger.debug('no sub-problem:{}'.format(current_prob))\n                continue\n\n            if len(current_prob.x_range) == 1 and len(current_prob.y_range) == 1:\n                # X# -> (| and _) or .\n                #  Y\n                if x == current_prob.x_range.stop and y == current_prob.y_range.stop:\n                    # right top point\n                    #        _\n                    # X# -> X  and  |\n                    #  Y     Y     Y\n                    sub_prob1 = ProblemData(\n                        x_range=range(current_prob.x_range.start, x),\n                        y_range=range(y, y),\n                    )\n                    sub_prob2 = ProblemData(\n                        x_range=range(x, x),\n                        y_range=range(current_prob.y_range.start, y),\n                    )\n                    self.add_problem(sub_prob1)\n                    self.add_problem(sub_prob2)\n                    logger.debug('two sub-problems:{}, (x,y)=({},{}) -> {},{}'.format(\n                        current_prob, x, y,\n                        sub_prob1, sub_prob2\n                    ))\n                    continue\n                else:  # x == current_prob.x_range.start and y == current_prob.y_range.start\n                    logger.debug('no sub-problem:{}'.format(current_prob))\n                    continue\n\n            if len(current_prob.x_range) == 1 and len(current_prob.y_range) > 1:\n                # X####### -> X   |\n                #     Y          Y\n                sub_prob = ProblemData(\n                    x_range=current_prob.x_range,\n                    y_range=range(y, y)\n                )\n                logger.debug('one row space, one sub-problem:{}, (x,y)=({},{}) -> {}'.format(\n                    current_prob, x, y,\n                    sub_prob,\n                ))\n                continue\n\n            if len(current_prob.x_range) > 1 and len(current_prob.y_range) == 1:\n                #  #     _\n                # X# -> X\n                #  #     Y\n                #  Y\n                sub_prob = ProblemData(\n                    x_range=range(x, x),\n                    y_range=current_prob.y_range,\n                )\n                logger.debug('one column space, one sub-problem:{}, (x,y)=({},{}) -> {}'.format(\n                    current_prob, x, y,\n                    sub_prob,\n                ))\n\n            if len(current_prob.x_range) >= 2 and len(current_prob.y_range) >= 2:\n                #  ###     ##\n                # X### -> X##  and X\n                #  ###                #\n                #   Y       Y        Y\n                sub_prob1 = ProblemData(\n                    x_range=range(current_prob.x_range.start, x),\n                    y_range=range(y, current_prob.y_range.stop),\n                )\n                sub_prob2 = ProblemData(\n                    x_range=range(x, current_prob.x_range.stop),\n                    y_range=range(current_prob.y_range.start, y),\n                )\n                self.add_problem(sub_prob1)\n                self.add_problem(sub_prob2)\n                logger.debug('two sub-problems:{}, (x,y)=({},{}) -> {},{}'.format(\n                    current_prob, x, y,\n                    sub_prob1, sub_prob2\n                ))\n                continue\n        if self.best_point is None:\n            logger.debug('no factorization is best')\n            return self.W, None, None\n\n        in_comp, out_comp = self.best_point\n        if in_comp >= self.in_dim:\n            in_comp = None\n        if out_comp >= self.out_dim:\n            out_comp = None\n        C, U_in, U_out = Tucker.factorize(self.W, in_comp, out_comp)\n        return C, U_in, U_out\n\n    def _find_edge_point(self, acceptable_error: float, current_prob: ProblemData) -> Optional[Tuple[int, int]]:\n        x_range = current_prob.x_range\n        y_range = current_prob.y_range\n\n        acceptable_points = []\n        # consider that acceptable point doesn't exist in the current_prob space.\n\n        while len(x_range) > 0 or len(y_range) > 0:\n            if len(x_range) in [0, 1]:\n                x = x_range.start\n            else:\n                x = round((x_range.start + x_range.stop) / 2)\n            if len(y_range) in [0, 1]:\n                y = y_range.start\n            else:\n                y = round((y_range.start + y_range.stop) / 2)\n\n            logger.debug('binary search (x,y)=({}, {}) x_range={} y_range={} prob={}'.format(\n                x, y, x_range, y_range, current_prob\n            ))\n\n            C, U_in, U_out = Tucker.factorize(self.W, x, y)\n\n            error = self.calc_error(self.W, C, U_in, U_out)\n            if error < acceptable_error:\n                logger.debug('binary search: under threshold={} error={}'.format(\n                    acceptable_error, error,\n                ))\n                acceptable_points.append((x, y))\n\n                # update ranges\n                x_range = range(x_range.start, x)\n                y_range = range(y_range.start, y)\n            else:\n                logger.debug('binary search: over threshold={} error={}'.format(\n                    acceptable_error, error,\n                ))\n\n                # update ranges\n                if x + 1 <= x_range.stop:\n                    new_x_start = x + 1\n                else:\n                    new_x_start = x\n                x_range = range(new_x_start, x_range.stop)\n\n                if y + 1 <= y_range.stop:\n                    new_y_start = y + 1\n                else:\n                    new_y_start = y\n                y_range = range(new_y_start, y_range.stop)\n        if len(acceptable_points) == 0:\n            return None\n        else:\n            return acceptable_points[-1]\n\n    @staticmethod\n    def calc_error(W: np.ndarray, C: np.ndarray, U_in: np.ndarray, U_out: np.ndarray) -> float:\n        \"\"\"calculate expected bound of error of output of layer\n        :param W: W x H x I x O\n        :param C: W x H x N x M\n        :param U_in: I x N\n        :param U_out: O x M\n        :return:\n        \"\"\"\n        W_hat = np.einsum('whnm,in,om->whio', C, U_in, U_out)\n        elemental_error = np.abs(W - W_hat)\n        error_bound = np.mean(elemental_error) / np.mean(np.abs(W))\n        return error_bound\n\n\nclass TuckerFactorizer(Factorizer):\n    factorize_target_layers = [Conv2D]\n\n    @classmethod\n    def compress(cls, old_layer: Conv2D, acceptable_error: float) -> Optional[FactorizedConv2DTucker]:\n        '''Compress layer's kernel 4D tensor using tucker decomposition under acceptable_error.\n        If it can't reduce the number of parameters, returns `None`.\n        :param old_layer:\n        :param acceptable_error:\n        :return:\n        '''\n        W = K.get_value(old_layer.kernel)\n        searcher = TuckerParamSearcher(W)\n        C, U_in, U_out = searcher.factorize_in_acceptable_error(acceptable_error)\n\n        kernel = C\n\n        if U_in is None and U_out is None:  # compression failed\n            return None\n\n        if U_in is None:\n            input_components = None\n            pre_kernel = None\n        else:\n            input_components = U_in.shape[1]\n            pre_kernel = U_in[np.newaxis, np.newaxis, :, :]\n\n        if U_out is None:\n            output_components = None\n            post_kernel = None\n        else:\n            output_components = U_out.shape[1]\n            post_kernel = U_out.T[np.newaxis, np.newaxis, :, :]\n\n        base_config = old_layer.get_config()\n\n        new_config = convert_config(\n            base_config,\n            ignore_args=[\n                'kernel_constraint',\n            ],\n            converts={\n                'kernel_regularizer': [\n                    'pre_kernel_regularizer',\n                    'kernel_regularizer',\n                    'post_kernel_regularizer',\n                ],\n                'kernel_initializer': [\n                    'pre_kernel_initializer',\n                    'kernel_initializer',\n                    'post_kernel_initializer',\n                ],\n            },\n            new_kwargs={\n                'input_components': input_components,\n                'output_components': output_components,\n            }\n        )\n\n        new_layer = FactorizedConv2DTucker(**new_config)\n        new_layer.build(old_layer.get_input_shape_at(0))  # to initialize weight variables\n\n        K.set_value(new_layer.kernel, kernel)\n\n        if pre_kernel is not None:\n            K.set_value(new_layer.pre_kernel, pre_kernel)\n        if post_kernel is not None:\n            K.set_value(new_layer.post_kernel, post_kernel)\n        return new_layer","execution_count":277,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nfrom collections import defaultdict\nfrom typing import Dict, List, Type\n\nfrom keras.engine import Layer, Model\n\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef compress(model: Model, acceptable_error: float,\n             factorizers=None) -> Model:\n    \"\"\"compress model under acceptable error\n    compress each model's layer by using given factorizers.\n    If the factorizer compress the layer, swap the layer and compressed layer by re-creating\n    node on computational graph.\n    :param model: Target model\n    :param acceptable_error: Layer-wize acceptable output error. If this value is smaller\n        the compressed model will be more accurate. The calculation process of this error is\n        depend on each factorizer. So see the implementation.\n    :param factorizers: Applicable factorizers. Factorizer factorize each layer if factorizer\n        can factorize the layer.\n    :return: Compressed model\n    \"\"\"\n    if factorizers is None:\n        factorizers = [SVDFactorizer, TuckerFactorizer]\n    layer2factorizers = defaultdict(list)  # type: Dict[Type[Layer], List[Type[Factorizer]]]\n    for fact in factorizers:\n        for layer in fact.factorize_target_layers:\n            layer2factorizers[layer].append(fact)\n\n    for layer_idx, layer in enumerate(model.layers):\n        layer_class = type(layer)\n        if layer_class not in layer2factorizers:\n            logger.info(\n                'factorizer not found layer:{!r}'.format(layer)\n            )\n            continue\n\n        new_layer = None\n        for factorizer in layer2factorizers[layer_class]:  # type: Factorizer\n            logger.info(\n                'factorizer found layer:{!r} factorizer:{!r}'.format(\n                    layer, factorizer,\n                )\n            )\n            new_layer = factorizer.compress(layer, acceptable_error)\n            if new_layer is None:  # failed factorization\n                logger.info(\n                    'factorization failed layer:{!r} factorizer:{!r}'.format(\n                        layer, factorizer,\n                    )\n                )\n                continue\n            else: # succeeded factorization\n                break\n\n        if new_layer is not None:\n            logger.info(\n                'swap old/new layer old_layer:{!r} new_layer{!r}'.format(\n                    layer, new_layer,\n                )\n            )\n            swap_layer_connection(layer, new_layer)\n            model.layers[layer_idx] = new_layer\n                      \n    new_model = Model(model.input, model.output)\n    new_model.compile(\n        optimizer=model.optimizer.__class__.__name__,  # TODO: improve here\n        # model.optimizer is instance of Optimizer and hold some variables for target model.\n        # Optimizer must be re-initialized, because compress function changes model structure.\n        loss=model.loss,\n        metrics=model.metrics,\n        loss_weights=model.loss_weights,\n        sample_weight_mode=model.sample_weight_mode,\n    )\n    return new_model","execution_count":278,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel1=model\n\n\nnew_model = compress(model1, 3e-1)\n#model.save('/kaggle/working/SvdTucker.h5')\nnew_model.summary()\nscore = new_model.evaluate(x_test, y_test,batch_size=10)\nprint(score[1])\n","execution_count":279,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"object of type 'Dropout' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-279-d40ad45b7fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#model.save('/kaggle/working/SvdTucker.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-278-053f4dc082aa>\u001b[0m in \u001b[0;36mcompress\u001b[0;34m(model, acceptable_error, factorizers)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     new_model.compile(\n\u001b[1;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# TODO: improve here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 241\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1432\u001b[0m                   \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                   \u001b[0mnode_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                   tensor_index=tensor_index)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;31m# Propagate to all previous tensors connected to this node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'Dropout' has no len()"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (new_model==model1):\n    print(\"yes\")\nelse: print(\"no\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}