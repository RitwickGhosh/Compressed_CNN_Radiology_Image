{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.applications.vgg16 import VGG16 as cnn\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras import layers\nfrom keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D\nfrom tensorflow.keras import Model\nfrom keras.layers import Input, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","execution_count":386,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/train'\nval_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/val'\ntest_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/test'\n# len(os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA'))","execution_count":387,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls /kaggle/input/chest-xray-pneumonia/chest_xray/test","execution_count":388,"outputs":[{"output_type":"stream","text":"\u001b[0m\u001b[01;34mNORMAL\u001b[0m/  \u001b[01;34mPNEUMONIA\u001b[0m/\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(os.listdir(train_dir + '/NORMAL')) + len(os.listdir(train_dir + '/PNEUMONIA'))\nprint(nb_train_samples)\nnb_val_samples  = len(os.listdir(val_dir + '/NORMAL')) + len(os.listdir(val_dir + '/PNEUMONIA'))\nprint(nb_val_samples)\nnb_test_samples  = len(os.listdir(test_dir + '/NORMAL')) + len(os.listdir(val_dir + '/PNEUMONIA'))\nprint(nb_test_samples)","execution_count":389,"outputs":[{"output_type":"stream","text":"5216\n16\n242\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1./255,\n                                      shear_range=0.2,zoom_range=0.2,\n                                      horizontal_flip=True)\n\nval_datagen = ImageDataGenerator(rescale = 1./255)\n\ntest_datagen = ImageDataGenerator(rescale=1./ 255)\n\nbatch_size = 10\nimg_width = 150\nimg_height = 150\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,batch_size = batch_size,\n                                                    class_mode = 'binary', \n                                                    target_size =(img_width, img_height))\n\nval_generator = val_datagen.flow_from_directory(val_dir,batch_size = batch_size,\n                                                    class_mode = 'binary', \n                                                    target_size =(img_width, img_height))\n\n\n\ntest_generator = test_datagen.flow_from_directory(test_dir,batch_size=batch_size,                                              \n                                                class_mode='binary',\n                                                target_size=(img_width, img_height))\n\n","execution_count":390,"outputs":[{"output_type":"stream","text":"Found 5216 images belonging to 2 classes.\nFound 16 images belonging to 2 classes.\nFound 624 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_train,y_train=train_generator.next()\nx_test,y_test=test_generator.next(624)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n# Define path to the data directory\ndata_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\nimport cv2\nfrom keras.utils import to_categorical\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'\n\n#Get the path to the sub-directories\nnormal_cases_dir = train_dir / 'NORMAL'\npneumonia_cases_dir = train_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\nimport numpy as np\n\n# Some images are in grayscale while majority of them contains 3 channels. So, if the image is grayscale, we will convert into a image with 3 channels.\n# We will normalize the pixel values and resizing all the images to 224x224 \n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150)).astype(np.float32)/255.0\n    label = to_categorical(0, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.0\n    label = to_categorical(1, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n# Convert the list into numpy arrays\nx_train = np.array(valid_data)\ny_train = np.array(valid_labels)\n","execution_count":391,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":392,"outputs":[{"output_type":"execute_result","execution_count":392,"data":{"text/plain":"(5216, 150, 150, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n# Define path to the data directory\ndata_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\nimport cv2\nfrom keras.utils import to_categorical\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'\n\n#Get the path to the sub-directories\nnormal_cases_dir = test_dir / 'NORMAL'\npneumonia_cases_dir = test_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\nimport numpy as np\n\n\n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150)).astype(np.float32)/255.0\n    label = to_categorical(0, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.0\n    label = to_categorical(1, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n# Convert the list into numpy arrays\nx_test = np.array(valid_data)\ny_test = np.array(valid_labels)\n","execution_count":393,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_test.shape","execution_count":394,"outputs":[{"output_type":"execute_result","execution_count":394,"data":{"text/plain":"(624, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_shape = Input(shape=(150,150,3))\nmodel = cnn(weights = 'imagenet',input_shape=(150,150,3), include_top=False)\n\nmodel.summary()","execution_count":395,"outputs":[{"output_type":"stream","text":"Model: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_27 (InputLayer)        [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now we will freeze all layers of our pretrained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:\n      layer.trainable = False","execution_count":396,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlast_output = model.output","execution_count":397,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)               \n# Add a final sigmoid layer for classification\nx = layers.Dense(2, activation='softmax')(x)           \n\nmodel = Model(model.input, x) \n\nmodel.summary()","execution_count":398,"outputs":[{"output_type":"stream","text":"Model: \"model_12\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_27 (InputLayer)        [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten_11 (Flatten)         (None, 8192)              0         \n_________________________________________________________________\ndense_24 (Dense)             (None, 1024)              8389632   \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 1024)              0         \n_________________________________________________________________\ndense_25 (Dense)             (None, 2)                 2050      \n=================================================================\nTotal params: 23,106,370\nTrainable params: 8,391,682\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model=cnn(weights=None, include_top=True,classes=2,input_shape=(150,150,3))\nmodel.summary()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sgd=SGD(lr=0.01,momentum=0.9,nesterov=True)\n#model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n\nmodel.compile(optimizer = Adam(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics = ['acc'])","execution_count":399,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"history = model.fit_generator(\n            train_generator,\n            validation_data = val_generator,\n            steps_per_epoch = nb_train_samples // batch_size,\n            epochs = 1,\n            validation_steps = nb_val_samples // batch_size,\n            verbose = 1)\n\nhis=model.fit(x_train, y_train, validation_split=0.2, epochs=1, batch_size=10)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory=model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=1, batch_size=10)\n","execution_count":400,"outputs":[{"output_type":"stream","text":"Train on 5216 samples, validate on 624 samples\n5216/5216 [==============================] - 14s 3ms/sample - loss: 0.1325 - acc: 0.9433 - val_loss: 0.5314 - val_acc: 0.8237\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","execution_count":401,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"score = model.evaluate(x_test, y_test,batch_size=10)\nprint(score[1])\n# evaluate the model\n#scores = model.evaluate_generator(test_generator)\n#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"/kaggle/working/vgg16model.h5\")","execution_count":402,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(score[1]*100)","execution_count":403,"outputs":[{"output_type":"stream","text":"82.37179517745972\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef prepare_softtargets(model,X):\n    inp = model.input                                           # input placeholder\n    outputs = []\n    for layer in model.layers[:]:\n        if layer.name == 'flatten_12':\n            outputs.append(layer.output)\n        if layer.name == 'dense_26':\n            outputs.append(layer.output)\n            \n    functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n    layer_outs = functor([X, 1])\n    return np.array(layer_outs[0]) , np.array(layer_outs[1])","execution_count":404,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lastconv_out = []\nlogit_out = []\nfor i in range(0,5):\n    \"\"\"l,l2 =  ( prepare_softtargets(model,x_train[i*100:(i+1)*100])\n    lastconv_out.append(l)\n    logit_out.append(l2)\"\"\"\n    lastconv_out , logit_out = prepare_softtargets(model,x_train[i*1000:(i+1)*1000])\n\n# lastconv_out.shape , logit_out.shape\nlastconv_out = np.array(lastconv_out)\nlogit_out = np.array(logit_out)\nprint (lastconv_out.shape)\nprint (logit_out.shape)\n#lastconv_out = lastconv_out.reshape((8192,10))\n#logit_out = logit_out.reshape((1024,10))\nprint (lastconv_out.shape)\nprint (logit_out.shape)\n\nprint (\"clean up \") \nx_train = 0","execution_count":417,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'int' object is not subscriptable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-417-f0046056e5ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlastconv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     logit_out.append(l2)\"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlastconv_out\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlogit_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_softtargets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# lastconv_out.shape , logit_out.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import h5py\nh5f = h5py.File('lastconv_out.h5', 'w')\nh5f.create_dataset('dataset_1', data=lastconv_out)\nh5f.close()\n\nh5f2 = h5py.File('logit_out.h5', 'w')\nh5f2.create_dataset('dataset_1', data=logit_out)\nh5f2.close()","execution_count":406,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# free up memory\nlastconv_out = 0\nlogit_out = 0 \n\ntest_lastconv_out = []\ntest_logit_out = []\n\nfor i in range(0,6):\n    print (\"Batch # : \",i)\n    l,l2 =  prepare_softtargets(model,x_test[i*100:(i+1)*100])\n   \n\n# lastconv_out.shape , logit_out.shape\ntest_lastconv_out = np.array(l)\ntest_logit_out = np.array(l2)\n\n#test_lastconv_out = test_lastconv_out.reshape((8192,10))\n#test_logit_out = test_logit_out.reshape((1024,10))\n\nprint (test_lastconv_out.shape)\nprint (test_logit_out.shape)","execution_count":407,"outputs":[{"output_type":"stream","text":"Batch # :  0\nBatch # :  1\nBatch # :  2\nBatch # :  3\nBatch # :  4\nBatch # :  5\nBatch # :  6\nBatch # :  7\nBatch # :  8\nBatch # :  9\nBatch # :  10\nBatch # :  11\nBatch # :  12\nBatch # :  13\nBatch # :  14\nBatch # :  15\n(10, 8192)\n(10, 1024)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"h5f = h5py.File('test_lastconv_out.h5', 'w')\nh5f.create_dataset('dataset_1', data=test_lastconv_out)\nh5f.close()\n\nh5f2 = h5py.File('test_logit_out.h5', 'w')\nh5f2.create_dataset('dataset_1', data=test_logit_out)\nh5f2.close()","execution_count":408,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax_c(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis]\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] \n    return e_x / div","execution_count":411,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logit Regression Method \nfrom sklearn import metrics\nresults = []\nfrom keras.layers import *\nfor HiddenNeuron in [2,3,4,5,6,7,9]:\n    \n    # Load input,target to studentModel \n    h5f = h5py.File('lastconv_out.h5' , 'r')\n    lastconv_out = h5f['dataset_1'][:]\n    h5f.close()\n    \n    h5f2 = h5py.File('logit_out.h5' , 'r')\n    logit_out = h5f2['dataset_1'][:]\n    h5f2.close()\n\n    student_model = Sequential()\n    student_model.add(Dense(HiddenNeuron,input_dim=8192,activation='relu'))\n    student_model.add(Dropout(0.2))\n    student_model.add(Dense(1024))\n\n    student_model.compile(loss='mse',\n                  optimizer=keras.optimizers.Adadelta(),\n                  metrics=['accuracy'])\n\n    student_model.fit(lastconv_out, logit_out,nb_epoch=40,verbose=0)\n#     student_model.save_weights(\"student_weights_\"+str(HiddenNeuron)+\"hidden_0.5_dropout.h5\")\n    \n    # Compression Rate from Number of Parameters Reduced\n    print (\"HiddenNeurons : \" , HiddenNeuron)\n    print (\"Initial Model Parameters : \" , model.count_params())\n    print (\"Compressed Model parameters: \", student_model.count_params())\n    \n    compressionRate = model.count_params() / np.float(student_model.count_params())\n    print (\"Compression Rate : \" , compressionRate)\n    \n    lastconv_out = 0\n    logit_out = 0                \n    \n    h5f = h5py.File('test_lastconv_out.h5' , 'r')\n    test_lastconv_out = h5f['dataset_1'][:]\n    h5f.close()\n    h5f2 = h5py.File('test_logit_out.h5' , 'r')\n    test_logit_out = h5f2['dataset_1'][:]\n    h5f2.close()\n    \n    pred = student_model.predict(test_lastconv_out)\n    probs = softmax_c(pred)\n    pred_classes = np.argmax(probs,axis=1)\n\n    accuracy_student = metrics.accuracy_score(y_pred=pred_classes,y_true=np.argmax(test_logit_out,axis=1))\n    print (\"Accuracy : \" , accuracy_student)\n                            \n    out = {\n        \"HiddenNeuron\" :    HiddenNeuron,\n        \"compressionRate\" : compressionRate,\n        \"nparams_student\" : student_model.count_params(),\n        \"accuracy_student\": accuracy_student\n    }\n    \n    student_model = 0 \n    lastconv_out = 0\n    logit_out = 0 \n    test_lastconv_out = 0\n    test_logit_out = 0 \n    \n    results.append(out)\n    # Free-up train Set","execution_count":415,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","name":"stderr"},{"output_type":"stream","text":"HiddenNeurons :  2\nInitial Model Parameters :  23106370\nCompressed Model parameters:  19458\nCompression Rate :  1187.4997430362832\nAccuracy :  0.2\nHiddenNeurons :  3\nInitial Model Parameters :  23106370\nCompressed Model parameters:  28675\nCompression Rate :  805.8019180470793\nAccuracy :  0.0\nHiddenNeurons :  4\nInitial Model Parameters :  23106370\nCompressed Model parameters:  37892\nCompression Rate :  609.7954713395967\nAccuracy :  0.0\nHiddenNeurons :  5\nInitial Model Parameters :  23106370\nCompressed Model parameters:  47109\nCompression Rate :  490.4873803307224\nAccuracy :  0.4\nHiddenNeurons :  6\nInitial Model Parameters :  23106370\nCompressed Model parameters:  56326\nCompression Rate :  410.22565067641943\nAccuracy :  0.0\nHiddenNeurons :  7\nInitial Model Parameters :  23106370\nCompressed Model parameters:  65543\nCompression Rate :  352.53757075507684\nAccuracy :  0.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":416,"outputs":[{"output_type":"execute_result","execution_count":416,"data":{"text/plain":"[{'HiddenNeuron': 2,\n  'compressionRate': 1187.4997430362832,\n  'nparams_student': 19458,\n  'accuracy_student': 0.2},\n {'HiddenNeuron': 3,\n  'compressionRate': 805.8019180470793,\n  'nparams_student': 28675,\n  'accuracy_student': 0.0},\n {'HiddenNeuron': 4,\n  'compressionRate': 609.7954713395967,\n  'nparams_student': 37892,\n  'accuracy_student': 0.0},\n {'HiddenNeuron': 5,\n  'compressionRate': 490.4873803307224,\n  'nparams_student': 47109,\n  'accuracy_student': 0.4},\n {'HiddenNeuron': 6,\n  'compressionRate': 410.22565067641943,\n  'nparams_student': 56326,\n  'accuracy_student': 0.0},\n {'HiddenNeuron': 7,\n  'compressionRate': 352.53757075507684,\n  'nparams_student': 65543,\n  'accuracy_student': 0.0}]"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}