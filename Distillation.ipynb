{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.applications.vgg16 import VGG16 as cnn\nfrom tensorflow.keras.optimizers import Adam,SGD\nfrom tensorflow.keras import layers\nfrom keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D\nfrom tensorflow.keras import Model\nfrom keras.layers import Input, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/train'\nval_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/val'\ntest_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray/test'\n# len(os.listdir('/kaggle/input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA'))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls /kaggle/input/chest-xray-pneumonia/chest_xray/test","execution_count":14,"outputs":[{"output_type":"stream","text":"\u001b[0m\u001b[01;34mNORMAL\u001b[0m/  \u001b[01;34mPNEUMONIA\u001b[0m/\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(os.listdir(train_dir + '/NORMAL')) + len(os.listdir(train_dir + '/PNEUMONIA'))\nprint(nb_train_samples)\nnb_val_samples  = len(os.listdir(val_dir + '/NORMAL')) + len(os.listdir(val_dir + '/PNEUMONIA'))\nprint(nb_val_samples)\nnb_test_samples  = len(os.listdir(test_dir + '/NORMAL')) + len(os.listdir(val_dir + '/PNEUMONIA'))\nprint(nb_test_samples)","execution_count":15,"outputs":[{"output_type":"stream","text":"5216\n16\n242\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1./255,\n                                      shear_range=0.2,zoom_range=0.2,\n                                      horizontal_flip=True)\n\nval_datagen = ImageDataGenerator(rescale = 1./255)\n\ntest_datagen = ImageDataGenerator(rescale=1./ 255)\n\nbatch_size = 10\nimg_width = 150\nimg_height = 150\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,batch_size = batch_size,\n                                                    class_mode = 'binary', \n                                                    target_size =(img_width, img_height))\n\nval_generator = val_datagen.flow_from_directory(val_dir,batch_size = batch_size,\n                                                    class_mode = 'binary', \n                                                    target_size =(img_width, img_height))\n\n\n\ntest_generator = test_datagen.flow_from_directory(test_dir,batch_size=batch_size,                                              \n                                                class_mode='binary',\n                                                target_size=(img_width, img_height))\n\n","execution_count":16,"outputs":[{"output_type":"stream","text":"Found 5216 images belonging to 2 classes.\nFound 16 images belonging to 2 classes.\nFound 624 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_train,y_train=train_generator.next()\nx_test,y_test=test_generator.next(624)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n# Define path to the data directory\ndata_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\nimport cv2\nfrom keras.utils import to_categorical\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'\n\n#Get the path to the sub-directories\nnormal_cases_dir = train_dir / 'NORMAL'\npneumonia_cases_dir = train_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\nimport numpy as np\n\n# Some images are in grayscale while majority of them contains 3 channels. So, if the image is grayscale, we will convert into a image with 3 channels.\n# We will normalize the pixel values and resizing all the images to 224x224 \n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150)).astype(np.float32)/255.0\n    label = to_categorical(0, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.0\n    label = to_categorical(1, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n# Convert the list into numpy arrays\nx_train = np.array(valid_data)\ny_train = np.array(valid_labels)\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"(5216, 150, 150, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\n# Define path to the data directory\ndata_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n\n# Path to train directory (Fancy pathlib...no more os.path!!)\ntrain_dir = data_dir / 'train'\nimport cv2\nfrom keras.utils import to_categorical\n# Path to validation directory\nval_dir = data_dir / 'val'\n\n# Path to test directory\ntest_dir = data_dir / 'test'\n\n#Get the path to the sub-directories\nnormal_cases_dir = test_dir / 'NORMAL'\npneumonia_cases_dir = test_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\nimport numpy as np\n\n\n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150)).astype(np.float32)/255.0\n    label = to_categorical(0, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (150,150))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.0\n    label = to_categorical(1, num_classes=2)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n# Convert the list into numpy arrays\nx_test = np.array(valid_data)\ny_test = np.array(valid_labels)\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_test.shape","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"(624, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_shape = Input(shape=(150,150,3))\nmodel = cnn(weights = 'imagenet',input_shape=(150,150,3), include_top=False)\n\nmodel.summary()","execution_count":21,"outputs":[{"output_type":"stream","text":"Model: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n=================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now we will freeze all layers of our pretrained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:\n      layer.trainable = False","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlast_output = model.output","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)               \n# Add a final sigmoid layer for classification\nx = layers.Dense(2, activation='softmax')(x)           \n\nmodel = Model(model.input, x) \n\nmodel.summary()","execution_count":24,"outputs":[{"output_type":"stream","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 150, 150, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1024)              8389632   \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2)                 2050      \n=================================================================\nTotal params: 23,106,370\nTrainable params: 8,391,682\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model=cnn(weights=None, include_top=True,classes=2,input_shape=(150,150,3))\nmodel.summary()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sgd=SGD(lr=0.01,momentum=0.9,nesterov=True)\n#model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n\nmodel.compile(optimizer = Adam(lr=0.0001), \n              loss = 'binary_crossentropy', \n              metrics = ['acc'])","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=5, batch_size=10)","execution_count":30,"outputs":[{"output_type":"stream","text":"Train on 5216 samples, validate on 624 samples\nEpoch 1/5\n5216/5216 [==============================] - 14s 3ms/sample - loss: 0.0830 - acc: 0.9705 - val_loss: 0.6188 - val_acc: 0.8189\nEpoch 2/5\n5216/5216 [==============================] - 13s 3ms/sample - loss: 0.0561 - acc: 0.9780 - val_loss: 1.3692 - val_acc: 0.7179\nEpoch 3/5\n5216/5216 [==============================] - 13s 3ms/sample - loss: 0.0442 - acc: 0.9824 - val_loss: 0.6488 - val_acc: 0.8301\nEpoch 4/5\n5216/5216 [==============================] - 13s 3ms/sample - loss: 0.0404 - acc: 0.9864 - val_loss: 1.4840 - val_acc: 0.7147\nEpoch 5/5\n5216/5216 [==============================] - 13s 3ms/sample - loss: 0.0340 - acc: 0.9887 - val_loss: 0.9844 - val_acc: 0.7837\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"history = model.fit_generator(\n            train_generator,\n            validation_data = val_generator,\n            steps_per_epoch = nb_train_samples // batch_size,\n            epochs = 5,\n            validation_steps = nb_val_samples // batch_size,\n            verbose = 1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 0 Axes>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n# evaluate the model\nscores = model.evaluate_generator(test_generator)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"/kaggle/working/mobilenetmodel.h5\")","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"score = model.evaluate(x_test, y_test, verbose=0)\nprint(score[1]*100)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ndef softmax_c(z):\n    assert len(z.shape) == 2\n    s = np.max(z, axis=1)\n    s = s[:, np.newaxis]\n    e_x = np.exp(z - s)\n    div = np.sum(e_x, axis=1)\n    div = div[:, np.newaxis] \n    return e_x / div","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef prepare_softtargets(model,X):\n    inp = model.input                                           # input placeholder\n    outputs = []\n    for layer in model.layers[:]:\n        if layer.name == 'flatten_1':\n            outputs.append(layer.output)\n        if layer.name == 'dense_2':\n            outputs.append(layer.output)\n            \n    functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n    layer_outs = functor([X, 1])\n    return np.array(layer_outs[0]) , np.array(layer_outs[1])","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_out = []\nlastconv_out = []\nfor i in range(0,10):\n    \"\"\"l,l2 =  ( prepare_softtargets(model,x_train[i*100:(i+1)*100])\n    lastconv_out.append(l)\n    logit_out.append(l2)\"\"\"\n    l,l2 = prepare_softtargets(model,x_train[i*521:(i+1)*521])\n    lastconv_out.append(l)\n    logit_out.append(l2)\n\n# lastconv_out.shape , logit_out.shape\nlastconv_out = np.array(lastconv_out)\nlogit_out = np.array(logit_out)\nlastconv_out = lastconv_out.reshape((5210 , 8192))\nlogit_out = logit_out.reshape((5210 , 1024))\nprint (lastconv_out.shape)\nprint (logit_out.shape)","execution_count":37,"outputs":[{"output_type":"stream","text":"(5210, 8192)\n(5210, 1024)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_lastconv_out = []\ntest_logit_out = []\n\nfor i in range(0,10):\n    print (\"Batch # : \",i)\n    l,l2 =  prepare_softtargets(model,x_test[i*62:(i+1)*62])\n    test_lastconv_out.append(l)\n    test_logit_out.append(l2)\n\n# lastconv_out.shape , logit_out.shape\ntest_lastconv_out = np.array(test_lastconv_out)\ntest_logit_out = np.array(test_logit_out)\n\ntest_lastconv_out = test_lastconv_out.reshape((620,8192))\ntest_logit_out = test_logit_out.reshape((620,1024))\n\nprint (test_lastconv_out.shape)\nprint (test_logit_out.shape)","execution_count":40,"outputs":[{"output_type":"stream","text":"Batch # :  0\nBatch # :  1\nBatch # :  2\nBatch # :  3\nBatch # :  4\nBatch # :  5\nBatch # :  6\nBatch # :  7\nBatch # :  8\nBatch # :  9\n(620, 8192)\n(620, 1024)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dropout, Dense\nfor HiddenNeuron in [1,2,10,100]:\n    student_model = Sequential()\n    student_model.add(Dense(HiddenNeuron,input_dim=8192,activation='relu'))\n    student_model.add(Dropout(0.2))\n    student_model.add(Dense(1024))\n\n    student_model.compile(loss='mse',\n                  optimizer=keras.optimizers.Adadelta(),\n                  metrics=['accuracy'])\n\n    student_model.fit(lastconv_out, logit_out,nb_epoch=40,verbose=0)\n#     student_model.save_weights(\"student_weights_\"+str(HiddenNeuron)+\"hidden_0.5_dropout.h5\")\n    scores = student_model.evaluate(test_lastconv_out,test_logit_out)\n    print(f\"Test Accuracy: {scores[1]*100}\")\n    # Compression Rate from Number of Parameters Reduced\n    print (\"HiddenNeurons : \" , HiddenNeuron)\n    print (\"Initial Model Parameters : \" , model.count_params())\n    print (\"Compressed Model parameters: \", student_model.count_params())\n    \n    compressionRate = model.count_params() / np.float(student_model.count_params())\n    print (\"Compression Rate : \" , compressionRate)\n    \n    pred = student_model.predict(test_lastconv_out)\n    probs = softmax_c(pred)\n    pred_classes = np.argmax(probs,axis=1)\n    accuracy_student = metrics.accuracy_score(y_pred=np.argmax(pred,axis=1),y_true=np.argmax(test_logit_out,axis=1))\n    #accuracy_student = metrics.accuracy_score(y_pred=pred_classes,y_true=np.argmax(test_logit_out,axis=1))\n    print (\"Accuracy : \" , accuracy_student)\n                            \n    out = {\n        \"HiddenNeuron\" :    HiddenNeuron,\n        \"compressionRate\" : compressionRate,\n        \"nparams_student\" : student_model.count_params(),\n        \"accuracy_student\": accuracy_student\n    }\n    \n  ","execution_count":41,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Sequential' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-4792df7ceb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mHiddenNeuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstudent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHiddenNeuron\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}